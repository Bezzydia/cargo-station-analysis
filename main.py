"""–¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞ –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ."""

import numpy as np
import tensorflow as tf
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
np.random.seed(42)
tf.random.set_seed(42)

# –ò–º–ø–æ—Ä—Ç –º–æ–¥—É–ª–µ–π
from src.data_loader import load_data, inspect_data
from src.data_processor import clean_and_prepare_data, create_features, create_lagged_features
from src.visualizer import (
    plot_time_series, plot_distributions, plot_correlation_matrix,
    plot_time_dependencies, plot_clustering_results, plot_feature_importance,
    plot_lstm_results, plot_anomaly_temporal_analysis
)
from src.analyzer import perform_clustering, analyze_anomalies
from src.modeler import (
    prepare_modeling_data, train_and_evaluate_regression_models,
    time_series_cv, train_lstm_model, train_svm_anomaly_model, save_models_and_results,
    evaluate_model # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ —Ñ–∏–Ω–∞–ª—å–Ω–æ–º –≤—ã–≤–æ–¥–µ
)

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è, –∑–∞–ø—É—Å–∫–∞—é—â–∞—è –≤–µ—Å—å –∞–Ω–∞–ª–∏–∑."""
    print("üöÄ –ù–ê–ß–ê–õ–û –ê–ù–ê–õ–ò–ó–ê –†–ê–ë–û–¢–´ –ñ–ï–õ–ï–ó–ù–û–î–û–†–û–ñ–ù–û–ô –°–¢–ê–ù–¶–ò–ò")
    print("=" * 50)

    # --- 1. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –∏–Ω—Å–ø–µ–∫—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö ---
    file_path = 'data/–í—Ö–æ–¥–Ω—ã–µ_–¥–∞–Ω–Ω—ã–µ.xlsx'
    df_raw = load_data(file_path)
    inspect_data(df_raw)

    # --- 2. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö ---
    df_clean_step1 = clean_and_prepare_data(df_raw)
    df_clean = create_features(df_clean_step1)

    # --- 3. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è (EDA) ---
    plot_time_series(df_clean)
    plot_distributions(df_clean)

    numeric_columns = [
        't(1) - –ü—Ä–∏–±—ã—Ç–∏–µ', 't(p) - –†–∞—Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ', 't(c) - –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞', 't(o) - –û—Ç–ø—Ä–∞–≤–ª–µ–Ω–∏–µ',
        '–ü—Ä–æ—Å—Ç–æ–π —Ñ–∞–∫—Ç', '–ü—Ä–æ—Å—Ç–æ–π –±–µ–∑ –ø–µ—Ä–µ—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–∫—Ç',
        '–ü—Ä–∏–±—ã—Ç–∏–µ –æ–±—â–µ–µ', '–û—Ç–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ–±—â–µ–µ',
        '–ü—Ä–∏–±—ã—Ç–∏–µ —Å –∑–∞–ø–∞–¥–∞', '–û—Ç–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –Ω–∞ –≤–æ—Å—Ç–æ–∫',
        '–ü—Ä–∏–±—ã—Ç–∏–µ —Å –≤–æ—Å—Ç–æ–∫–∞', '–û—Ç–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –Ω–∞ –∑–∞–ø–∞–¥',
        '–†–∞–±–æ—á–∏–π –ø–∞—Ä–∫', '–†–∞–±–æ—á–∏–π –ø–∞—Ä–∫ –Ω–µ—á–µ—Ç–Ω—ã–π', '–†–∞–±–æ—á–∏–π –ø–∞—Ä–∫ —á–µ—Ç–Ω—ã–π',
        '–¥–∏—Å–±–∞–ª–∞–Ω—Å_–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π', '–ø–µ—Ä–µ—Ä–∞–±–æ—Ç–∫–∞', '–æ—Ç–Ω–æ—à–µ–Ω–∏–µ_–ø—Ä–∏–±—ã—Ç–∏–µ_–æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∏–µ',
        '–ß–∞—Å', '–î–µ–Ω—å_–Ω–µ–¥–µ–ª–∏', '–ú–µ—Å—è—Ü'
    ]
    corr_matrix = plot_correlation_matrix(df_clean, numeric_columns)

    # --- 4. –ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π (–ª–∞–≥–∏) ---
    target_columns = ['–ü—Ä–æ—Å—Ç–æ–π —Ñ–∞–∫—Ç', '–†–∞–±–æ—á–∏–π –ø–∞—Ä–∫', '–ü—Ä–∏–±—ã—Ç–∏–µ –æ–±—â–µ–µ', '–û—Ç–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ–±—â–µ–µ', 't(c) - –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞']
    df_lagged = create_lagged_features(df_clean, target_columns, lags=[1, 2, 3])
    # –ü–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –¥–ª—è –¥–∞–Ω–Ω—ã—Ö —Å –ª–∞–≥–∞–º–∏, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    # corr_matrix_lagged = df_lagged.corr() 
    plot_time_dependencies(df_lagged)

    # --- 5. –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è ---
    MAX_SAMPLE_SIZE = 2000
    df_sample = df_clean.iloc[np.random.choice(len(df_clean), size=min(MAX_SAMPLE_SIZE, len(df_clean)), replace=False)].copy() if len(df_clean) > MAX_SAMPLE_SIZE else df_clean.copy()

    clustering_features = [
        't(1) - –ü—Ä–∏–±—ã—Ç–∏–µ', 't(c) - –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞', '–ü—Ä–æ—Å—Ç–æ–π —Ñ–∞–∫—Ç',
        '–†–∞–±–æ—á–∏–π –ø–∞—Ä–∫', '–¥–∏—Å–±–∞–ª–∞–Ω—Å_–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π', '–ø–µ—Ä–µ—Ä–∞–±–æ—Ç–∫–∞',
        '–ü—Ä–∏–±—ã—Ç–∏–µ –æ–±—â–µ–µ', '–û—Ç–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ–±—â–µ–µ'
    ]
    df_with_clusters, cluster_analysis, centers_idx, clusters, optimal_ra = perform_clustering(
        df_sample, df_clean, clustering_features, MAX_SAMPLE_SIZE
    )
    plot_clustering_results(df_with_clusters)

    # --- 6. –ê–Ω–∞–ª–∏–∑ –∞–Ω–æ–º–∞–ª–∏–π ---
    df_with_anomalies = analyze_anomalies(df_with_clusters)
    # –î–æ–±–∞–≤–∏–º –∞–Ω–æ–º–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ df_clean –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    df_clean['–ê–Ω–æ–º–∞–ª—å–Ω—ã–π_–ø—Ä–æ—Å—Ç–æ–π'] = df_with_anomalies['–ê–Ω–æ–º–∞–ª—å–Ω—ã–π_–ø—Ä–æ—Å—Ç–æ–π']
    df_clean['–ê–Ω–æ–º–∞–ª—å–Ω—ã–π_–ø–∞—Ä–∫'] = df_with_anomalies['–ê–Ω–æ–º–∞–ª—å–Ω—ã–π_–ø–∞—Ä–∫']

    # --- 7. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è ---
    (X_train_dt, X_test_dt, y_train_dt, y_test_dt,
     X_train_park, X_test_park, y_train_park, y_test_park) = prepare_modeling_data(df_clean)

    # --- 8. –ú–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ (–†–µ–≥—Ä–µ—Å—Å–∏—è) ---
    models_dt, models_park, metrics_dt, metrics_park = train_and_evaluate_regression_models(
        X_train_dt, X_test_dt, y_train_dt, y_test_dt,
        X_train_park, X_test_park, y_train_park, y_test_park
    )

    # --- 9. –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è ---
    print("\n=== –ö–†–û–°–°-–í–ê–õ–ò–î–ê–¶–ò–Ø –° –í–†–ï–ú–ï–ù–ù–´–ú –†–ê–ó–ë–ò–ï–ù–ò–ï–ú ===")
    # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ —É –Ω–∞—Å –µ—Å—Ç—å —Å–∫–∞–ª–µ—Ä—ã –∏–∑ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è
    scaler_dt = models_dt['Ridge'][1] # –ü–æ–ª—É—á–∞–µ–º scaler –∏–∑ –∫–æ—Ä—Ç–µ–∂–∞ (model, scaler)
    X_train_dt_scaled = scaler_dt.transform(X_train_dt) if scaler_dt else X_train_dt.values
    scaler_park = models_park['Ridge'][1]
    X_train_park_scaled = scaler_park.transform(X_train_park) if scaler_park else X_train_park.values

    cv_results = {}
    for name, (model, _) in models_dt.items():
        X_for_cv = X_train_dt_scaled if name == 'Ridge' else X_train_dt.values
        cv_results[f'{name}_downtime'] = time_series_cv(model, X_for_cv, y_train_dt, name)

    for name, (model, _) in models_park.items():
        X_for_cv = X_train_park_scaled if name == 'Ridge' else X_train_park.values
        cv_results[f'{name}_park'] = time_series_cv(model, X_for_cv, y_train_park, name)

    # --- 10. –ú–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ (LSTM) ---
    lstm_features = [
        't(1) - –ü—Ä–∏–±—ã—Ç–∏–µ', 't(c) - –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞', '–ü—Ä–∏–±—ã—Ç–∏–µ –æ–±—â–µ–µ', '–û—Ç–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ–±—â–µ–µ',
        '–†–∞–±–æ—á–∏–π –ø–∞—Ä–∫ –Ω–µ—á–µ—Ç–Ω—ã–π', '–†–∞–±–æ—á–∏–π –ø–∞—Ä–∫ —á–µ—Ç–Ω—ã–π', '–¥–∏—Å–±–∞–ª–∞–Ω—Å_–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π', '–ü—Ä–æ—Å—Ç–æ–π —Ñ–∞–∫—Ç'
    ]
    lstm_model, history, lstm_preds, lstm_metrics, scaler_lstm = train_lstm_model(df_clean, lstm_features, epochs=10)
    y_test_inv, test_predict_inv = lstm_preds
    mae_lstm, rmse_lstm, r2_lstm = lstm_metrics
    plot_lstm_results(history, y_test_inv, test_predict_inv)

    # --- 11. –ú–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ (–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∞–Ω–æ–º–∞–ª–∏–π - SVM) ---
    features_initial_dt = [
        't(1) - –ü—Ä–∏–±—ã—Ç–∏–µ', 't(p) - –†–∞—Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ', 't(c) - –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞', 't(o) - –û—Ç–ø—Ä–∞–≤–ª–µ–Ω–∏–µ',
        '–ü—Ä–∏–±—ã—Ç–∏–µ –æ–±—â–µ–µ', '–û—Ç–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ–±—â–µ–µ',
        '–ü—Ä–∏–±—ã—Ç–∏–µ —Å –∑–∞–ø–∞–¥–∞', '–û—Ç–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –Ω–∞ –≤–æ—Å—Ç–æ–∫',
        '–ü—Ä–∏–±—ã—Ç–∏–µ —Å –≤–æ—Å—Ç–æ–∫–∞', '–û—Ç–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –Ω–∞ –∑–∞–ø–∞–¥',
        '–†–∞–±–æ—á–∏–π –ø–∞—Ä–∫ –Ω–µ—á–µ—Ç–Ω—ã–π', '–†–∞–±–æ—á–∏–π –ø–∞—Ä–∫ —á–µ—Ç–Ω—ã–π',
        '–¥–∏—Å–±–∞–ª–∞–Ω—Å_–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π'
    ]
    svm_dt, scaler_an_dt, svm_preds, feature_importance_svm = train_svm_anomaly_model(df_clean, features_initial_dt)
    y_test_an, y_pred_an = svm_preds
    plot_feature_importance(feature_importance_svm.tail(10), '–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∞–Ω–æ–º–∞–ª–∏–π –ø—Ä–æ—Å—Ç–æ—è (Permutation Importance)', 'svm_feature_importance.png')

    # --- 12. –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π ---
    # –î–ª—è Random Forest –ø—Ä–æ—Å—Ç–æ—è
    rf_dt_model = models_dt['Random Forest'][0]
    feature_importance_dt = pd.Series(rf_dt_model.feature_importances_, index=X_train_dt.columns)
    plot_feature_importance(feature_importance_dt.sort_values(ascending=False).head(10), '–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ—Å—Ç–æ—è', 'feature_importance_downtime.png')

    # –î–ª—è Random Forest –ø–∞—Ä–∫–∞
    rf_park_model = models_park['Random Forest'][0]
    feature_importance_park = pd.Series(rf_park_model.feature_importances_, index=X_train_park.columns)
    plot_feature_importance(feature_importance_park.sort_values(ascending=False).head(10), '–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–∞–±–æ—á–µ–≥–æ –ø–∞—Ä–∫–∞', 'feature_importance_park.png')

    # --- 13. –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∞–Ω–æ–º–∞–ª–∏–π ---
    plot_anomaly_temporal_analysis(df_clean)

    # --- 14. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ---
    cluster_data_to_save = {'centers': centers_idx, 'clusters': clusters, 'ra': optimal_ra}
    save_models_and_results(
        models_dt, models_park, svm_dt, scaler_an_dt,
        lstm_model, scaler_lstm, cluster_data_to_save
    )

    # --- 15. –§–∏–Ω–∞–ª—å–Ω—ã–π –≤—ã–≤–æ–¥ –∏ –æ—Ç—á–µ—Ç ---
    print("\n=== –§–ò–ù–ê–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ê–ù–ê–õ–ò–ó–ê ===")
    
    # 1. –ö–õ–Æ–ß–ï–í–´–ï –ó–ê–í–ò–°–ò–ú–û–°–¢–ò
    print("1. –ö–õ–Æ–ß–ï–í–´–ï –ó–ê–í–ò–°–ò–ú–û–°–¢–ò:")
    correlation_with_downtime = corr_matrix['–ü—Ä–æ—Å—Ç–æ–π —Ñ–∞–∫—Ç'].abs().sort_values(ascending=False)
    correlation_with_park = corr_matrix['–†–∞–±–æ—á–∏–π –ø–∞—Ä–∫'].abs().sort_values(ascending=False)
    
    print("  –ù–∞–∏–±–æ–ª–µ–µ —Å–∏–ª—å–Ω—ã–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ —Å –ø—Ä–æ—Å—Ç–æ–µ–º:")
    for feature, corr in correlation_with_downtime.head(6).items():
        if feature != '–ü—Ä–æ—Å—Ç–æ–π —Ñ–∞–∫—Ç':
            print(f"   - {feature}: {corr:.3f}")
            
    print("\n  –ù–∞–∏–±–æ–ª–µ–µ —Å–∏–ª—å–Ω—ã–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ —Å —Ä–∞–±–æ—á–∏–º –ø–∞—Ä–∫–æ–º:")
    for feature, corr in correlation_with_park.head(6).items():
        if feature != '–†–∞–±–æ—á–∏–π –ø–∞—Ä–∫':
            print(f"   - {feature}: {corr:.3f}")

    # 2. –õ–£–ß–®–ò–ï –ú–û–î–ï–õ–ò –ü–†–û–ì–ù–û–ó–ò–†–û–í–ê–ù–ò–Ø
    print("\n2. –õ–£–ß–®–ò–ï –ú–û–î–ï–õ–ò –ü–†–û–ì–ù–û–ó–ò–†–û–í–ê–ù–ò–Ø:")
    best_dt_model_name = max(metrics_dt, key=lambda k: metrics_dt[k][2]) # R2 is index 2
    best_dt_metrics = metrics_dt[best_dt_model_name]
    print(f"  –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ—Å—Ç–æ—è:")
    print(f"   - –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_dt_model_name}")
    print(f"   - R¬≤: {best_dt_metrics[2]:.4f}, MAE: {best_dt_metrics[0]:.2f}")

    best_park_model_name = max(metrics_park, key=lambda k: metrics_park[k][2])
    best_park_metrics = metrics_park[best_park_model_name]
    print(f"\n  –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–±–æ—á–µ–≥–æ –ø–∞—Ä–∫–∞:")
    print(f"   - –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_park_model_name}")
    print(f"   - R¬≤: {best_park_metrics[2]:.4f}, MAE: {best_park_metrics[0]:.2f}")

    print(f"\n  LSTM –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ—Å—Ç–æ—è:")
    print(f"   - R¬≤: {r2_lstm:.4f}, MAE: {mae_lstm:.2f}")

    # 3. –ê–ù–ê–õ–ò–ó –ê–ù–û–ú–ê–õ–ò–ô
    print("\n3. –ê–ù–ê–õ–ò–ó –ê–ù–û–ú–ê–õ–ò–ô:")
    anomaly_rate_dt = df_clean['–ê–Ω–æ–º–∞–ª—å–Ω—ã–π_–ø—Ä–æ—Å—Ç–æ–π'].mean()
    anomaly_rate_park = df_clean['–ê–Ω–æ–º–∞–ª—å–Ω—ã–π_–ø–∞—Ä–∫'].mean()
    f1_svm = f1_score(y_test_an, y_pred_an)
    print(f"   - –ê–Ω–æ–º–∞–ª–∏–∏ –ø—Ä–æ—Å—Ç–æ—è (> 15 —á–∞—Å–æ–≤): {anomaly_rate_dt:.1%} –¥–∞–Ω–Ω—ã—Ö")
    print(f"   - –ê–Ω–æ–º–∞–ª–∏–∏ —Ä–∞–±–æ—á–µ–≥–æ –ø–∞—Ä–∫–∞ (> 5000 –≤–∞–≥.): {anomaly_rate_park:.1%} –¥–∞–Ω–Ω—ã—Ö")
    print(f"   - SVM –ø–æ–∫–∞–∑–∞–ª –≤—ã—Å–æ–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –≤ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–∏ –∞–Ω–æ–º–∞–ª–∏–π –ø—Ä–æ—Å—Ç–æ—è (F1-score: {f1_svm:.2f})")

    print("\n‚úÖ –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù –£–°–ü–ï–®–ù–û!")
    print("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –º–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫–µ 'results/'")

if __name__ == '__main__':
    main()
